{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem.snowball import  SnowballStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data from Train and Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(name):\n",
    "    text, targets = [], []\n",
    "\n",
    "    with open('data/{}.csv'.format(name)) as f:\n",
    "        for item in DictReader(f):\n",
    "            text.append(item['text'].decode('utf8'))\n",
    "            targets.append(item['category'])\n",
    "\n",
    "    return text, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built function for preprocessing. Removal of stopwords for each line. Afterwards, usedSnowballStemmer to stem each of the individual  words in a given line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(line):\n",
    "    stemmer=SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    line=line.split()\n",
    "    words=\"\"\n",
    "    for i in line:\n",
    "        if i not in stop_words:\n",
    "            words=words+stemmer.stem(i)+\" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built clean_str function to clean the strings with word contractions. Using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "\n",
    "\n",
    "\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model using Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1: 0.323676828408\n"
     ]
    }
   ],
   "source": [
    "text_train, targets_train = read_data('train')\n",
    "text_test, targets_test = read_data('test')\n",
    "        \n",
    "model = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            LogisticRegression(),\n",
    "        ).fit(text_train, targets_train)\n",
    "        \n",
    "prediction = model.predict(text_test)\n",
    "        \n",
    "baseline = f1_score(targets_test, prediction, average='macro')\n",
    "print 'macro f1:', baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the clean string function and stem function to each Whisper \n",
    "Also printing a small set of clean strings of Whispers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'look someon hold convers guy ', u'hey ', u'singl guy girl near spald ? femal ', u'white girl like ? ', u\"n't feel enthusiast anyth life anymor realli suck \", u'swear god , husband suggest give dog away one time give away ', u'gay bi guy delhi ? ? ', u'someon road trip mall ', u'realli like hispan girl class thug girl ', u\"bought puppi fix relationship 's still heroin addict \", u'someon stop way work ask number flatter yet terrifi moment life ', u\"gonna mile 277 tonight see well hungarian line danc anyon wanna come 's gonna fun time 21m \", u'anybodi wanna talk ? extrem bore ', u\"never date valentin 's day hate hear peopl complain ! 's realli big deal \", u'get guy valentin day ? ', u'need white chocol life ', u'english hello french bonjour spanish hola sup bitch ', u'im feel realli bad tonight ', u\"'m 19 n't know meet girl want relationship advic ? \", u'mommi best ']\n"
     ]
    }
   ],
   "source": [
    "text_train=map(clean_str,text_train)\n",
    "text_test=map(clean_str,text_test)\n",
    "\n",
    "text_train=map(stem_words,text_train)\n",
    "text_test=map(stem_words,text_test)\n",
    "\n",
    "print text_train[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Logistic Regression with changed parameter for min_df=20; Igonring terms with document freq less than 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 using TFIDF: 0.393336680879\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression using Tfidf\n",
    "model_tf = make_pipeline(TfidfVectorizer(max_df=0.95, min_df=20,\n",
    "                                stop_words='english'),\n",
    "        LogisticRegression(),\n",
    "    ).fit(text_train, targets_train)\n",
    "\n",
    "prediction = model_tf.predict(text_test)\n",
    "score_logit = f1_score(targets_test, prediction, average='macro')\n",
    "print 'macro f1 using TFIDF:', score_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Logistic Regression with CountVectorizer instead of TFIDF, and min_df as 7 we can see some improvement in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 using CountVectorizer: 0.483082746713\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression using CountVectorizer\n",
    "model_count = make_pipeline(CountVectorizer(max_df=0.95, min_df=7,\n",
    "                                stop_words='english'),\n",
    "        LogisticRegression(),\n",
    "    ).fit(text_train, targets_train)\n",
    "\n",
    "prediction = model_count.predict(text_test)\n",
    "\n",
    "print 'macro f1 using CountVectorizer:', f1_score(targets_test, prediction, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes model for text classification. Trying out for different values of min_df. It seems to works best for min_df=30 for countvectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 using CountVectorizer: 5 0.35282880051\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 0.368736090016\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20 0.384899707225\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30 0.397222581241\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40 0.369767241368\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50 0.351785534404\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60 0.330865655264\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65 0.328499181393\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70 0.287506060653\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75 0.283798853677\nmacro f1 using CountVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80 0.273991831072\n"
     ]
    }
   ],
   "source": [
    "min=[5,10,20,30,40,50,60,65,70,75,80]\n",
    "for n in min:\n",
    "    model_NB = make_pipeline(CountVectorizer(max_df=0.95, min_df=n,\n",
    "                                stop_words='english'),\n",
    "        MultinomialNB(),\n",
    "    ).fit(text_train, targets_train)\n",
    "\n",
    "    prediction = model_NB.predict(text_test)\n",
    "\n",
    "    print 'macro f1 using CountVectorizer:',n, f1_score(targets_test, prediction, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Naive Bayes with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 using TFIDFVectorizer: 5 0.147993473451\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 0.160772178259\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20 0.188435513836\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30 0.202469326632\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40 0.213223046973\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50 0.219771729656\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60 0.226549371907\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65 0.226847082885\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70 0.199120890314\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75 0.194391932908\nmacro f1 using TFIDFVectorizer:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80 0.193920631685\n"
     ]
    }
   ],
   "source": [
    "min=[5,10,20,30,40,50,60,65,70,75,80]\n",
    "for n in min:\n",
    "    model_NB = make_pipeline(TfidfVectorizer(max_df=0.95, min_df=n\n",
    "                                         ,\n",
    "                                stop_words='english'),\n",
    "        MultinomialNB(),\n",
    "    ).fit(text_train, targets_train)\n",
    "\n",
    "    prediction = model_NB.predict(text_test)\n",
    "\n",
    "    print 'macro f1 using TFIDFVectorizer:',n, f1_score(targets_test, prediction, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier using TFIDF and Count Vectorizer respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.373191943857\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing Words\n",
    "#Random Forest using TFIDF\n",
    "vect= TfidfVectorizer(stop_words='english',min_df=6)\n",
    "text_train_tf=vect.fit_transform(text_train)\n",
    "text_test_tf=vect.transform(text_test)\n",
    "rand_class=RandomForestClassifier()\n",
    "rand_class.fit(text_train_tf,targets_train)\n",
    "predi=rand_class.predict(text_test_tf)\n",
    "print 'Random Forest:', f1_score(targets_test, predi, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.380043821258\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing Words\n",
    "#Random Forest using CountVectorizer\n",
    "vect2= CountVectorizer(stop_words='english',min_df=6)\n",
    "text_train_cf=vect2.fit_transform(text_train)\n",
    "text_test_cf=vect2.transform(text_test)\n",
    "rand_class=RandomForestClassifier()\n",
    "rand_class.fit(text_train_cf,targets_train)\n",
    "predi=rand_class.predict(text_test_cf)\n",
    "print 'Random Forest:', f1_score(targets_test, predi, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning Random Forest Classifier with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'min_samples_split': 15, 'n_estimators': 350} 0.552747722096 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=1,\n            min_samples_split=15, min_weight_fraction_leaf=0.0,\n            n_estimators=350, n_jobs=1, oob_score=False, random_state=None,\n            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "tuned_parameters = [{'n_estimators':[150,200,250,300,350], 'min_samples_split':[15,25,50]}]\n",
    "gs_clf = GridSearchCV(RandomForestClassifier(), tuned_parameters,cv=5, n_jobs=-1)\n",
    "gs_clf.fit(text_train_tf,targets_train)\n",
    "print \"Best parameter\" , gs_clf.best_params_,gs_clf.best_score_,gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using results from GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best: 0.402159141802\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Best Parameters\n",
    "random_clf=RandomForestClassifier(n_estimators=350,min_samples_split=15)\n",
    "random_clf.fit(text_train_tf,targets_train)\n",
    "predict_random=random_clf.predict(text_test_tf)\n",
    "random_score = f1_score(targets_test, predict_random, average='macro')\n",
    "print 'Random Forest Best:', random_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for Support Vector Machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'penalty': 'l2', 'loss': 'hinge', 'C': 1} 0.545842824601 LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n     penalty='l2', random_state=None, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine - GridSearchCV\n",
    "# GridSearch CV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "tuned_parameters = [{'C': [1,10,20,50] ,'penalty': ['l2'],'loss':['hinge','squared_hinge']}]\n",
    "gs_clf = GridSearchCV(LinearSVC(), tuned_parameters,cv=5, n_jobs=-1)\n",
    "gs_clf.fit(text_train_tf,targets_train)\n",
    "print \"Best parameter\" , gs_clf.best_params_,gs_clf.best_score_,gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LinearSVC : 0.501105917658\n"
     ]
    }
   ],
   "source": [
    "#Best model LinearSVC\n",
    "best_linear_svc=LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
    "intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
    "best_linear_svc.fit(text_train_tf,targets_train)\n",
    "best_linear_svc_predi=best_linear_svc.predict(text_test_tf)\n",
    "svc_score = f1_score(targets_test, best_linear_svc_predi, average='macro')\n",
    "print 'Best LinearSVC :', svc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Classifier with TfidfVectorizer. \n",
    "Doesn't fit the data well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN fit:  2 0.268586588958\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 0.284336328812\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 0.236543032901\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5 0.228933058943\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6 0.19878931\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7 0.189799640607\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8 0.174744809253\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20 0.107783719942\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50 0.0510031011409\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100 0.0292197971398\n"
     ]
    }
   ],
   "source": [
    "#KNN Classifier using TFIDF vectorizer\n",
    "knn_list=[2,3,4,5,6,7,8,20,50,100]\n",
    "for n in knn_list:\n",
    "    knn_clf=KNeighborsClassifier(n_neighbors=n)\n",
    "    knn_clf.fit(text_train_tf,targets_train)\n",
    "    knn_predict=knn_clf.predict(text_test_tf)\n",
    "    print 'KNN fit: ',n , f1_score(targets_test, knn_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN fit:  2 0.228210756002\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 0.239746127845\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 0.224608927481\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5 0.221215773499\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6 0.205514690991\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7 0.196389785237\nKNN fit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8 0.177747127535\n"
     ]
    }
   ],
   "source": [
    "#KNN Classifier using CountVectorizer\n",
    "knn_list=[2,3,4,5,6,7,8]\n",
    "for n in knn_list:\n",
    "    knn_clf=KNeighborsClassifier(n_neighbors=n)\n",
    "    knn_clf.fit(text_train_cf,targets_train)\n",
    "    knn_predict=knn_clf.predict(text_test_tf)\n",
    "    print 'KNN fit: ',n , f1_score(targets_test, knn_predict, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying an ensemble of models using Voting Classifier in sklearn. Applied CountVectorizer. Used soft voting, so it takes prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble results: 0.485610066809\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.484682264313\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.481999490229\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.485841864224\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.483958775745\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.482591796619\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.471225785534\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.453485102442\n"
     ]
    }
   ],
   "source": [
    "#Building an Ensemble model using Voting classifier\n",
    "   \n",
    "vect3= CountVectorizer(stop_words='english',min_df=5,ngram_range=(1,2))\n",
    "text_train_en=vect3.fit_transform(text_train)\n",
    "text_test_en=vect3.transform(text_test)\n",
    "    \n",
    "    \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators = []\n",
    "clf1=SVC(C=1,kernel='linear',probability=True)\n",
    "clf2=RandomForestClassifier(n_estimators=200,min_samples_split=25)\n",
    "clf3=LogisticRegression()\n",
    "clf4=MultinomialNB()\n",
    "estimators.append(('SVM',clf1))\n",
    "estimators.append(('Random Forest',clf2))\n",
    "estimators.append(('Naive Bayes',clf4))\n",
    "estimators.append(('Logistic Regression',clf3))\n",
    "ensemble = VotingClassifier(estimators, voting='soft', weights=[3,1,1,2])\n",
    "ensemble=ensemble.fit(text_train_en,targets_train)\n",
    "prediction_ensemble=ensemble.predict(text_test_en)\n",
    "print 'ensemble results:', f1_score(targets_test, prediction_ensemble, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out an ensemble of models. This time with TFIDF. Tried out with different min_df to get the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble results: 0.522485402096\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.523735017437\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.529372460615\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.523632802102\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.522769504949\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.522706246345\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.49936838823\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.485655593514\nensemble results:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.470469387258\n"
     ]
    }
   ],
   "source": [
    "#Building an Ensemble model using Voting classifier\n",
    "min=[2,3,4,5,6,7,8,9,10]\n",
    "for n in min:\n",
    "    vect4= TfidfVectorizer(stop_words='english',min_df=n,ngram_range=(1,2))\n",
    "    text_train_en=vect4.fit_transform(text_train)\n",
    "    text_test_en=vect4.transform(text_test)\n",
    "        \n",
    "        \n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    estimators = []\n",
    "    clf1=SVC(C=1,kernel='linear',probability=True)\n",
    "    clf2=RandomForestClassifier(n_estimators=200,min_samples_split=25)\n",
    "    clf3=LogisticRegression()\n",
    "    clf4=MultinomialNB()\n",
    "    estimators.append(('SVM',clf1))\n",
    "    estimators.append(('Random Forest',clf2))\n",
    "    # estimators.append(('Naive Bayes',clf4))\n",
    "    estimators.append(('Logistic Regression',clf3))\n",
    "    ensemble = VotingClassifier(estimators, voting='soft', weights=[2,1,1])\n",
    "    ensemble=ensemble.fit(text_train_en,targets_train)\n",
    "    prediction_ensemble=ensemble.predict(text_test_en)\n",
    "    print 'ensemble results:', f1_score(targets_test, prediction_ensemble, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code I used to try out different weights of the classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying all weights\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import cross_validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "np.random.seed(123)\n",
    "    \n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w3', 'mean', 'std'))\n",
    "    \n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "    \n",
    "            if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n",
    "                continue\n",
    "    \n",
    "            eclf = VotingClassifier(estimators,voting='soft', weights=[w1,w2,w3])\n",
    "            scores = cross_validation.cross_val_score(\n",
    "                                                estimator=eclf,\n",
    "                                                X=text_train_tf,\n",
    "                                                y=targets_train,\n",
    "                                                cv=5,\n",
    "                                                scoring='accuracy',\n",
    "                                                n_jobs=1)\n",
    "    \n",
    "            df.loc[i] = [w1, w2, w3, scores.mean(), scores.std()]\n",
    "            i += 1\n",
    "    \n",
    "df.sort(columns=['mean', 'std'], ascending=False)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final ensemble model. Using TFIDF with min_df = 4 and used bigram to also include word pairs. \n",
    "Displaying the final vocabulary and stop words created by TFIDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_ensemble= TfidfVectorizer(stop_words='english',min_df=4,ngram_range=(1,2))\n",
    "text_train_en=vect_ensemble.fit_transform(text_train)\n",
    "text_test_en=vect_ensemble.transform(text_test)\n",
    "# print vect_ensemble.vocabulary_\n",
    "# print vect_ensemble.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble results: 0.531868138647\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators = []\n",
    "clf1=SVC(C=1,kernel='linear',probability=True)\n",
    "clf2=RandomForestClassifier(n_estimators=350,min_samples_split=15)\n",
    "clf3=LogisticRegression()\n",
    "estimators.append(('SVM',clf1))\n",
    "estimators.append(('Random Forest',clf2))\n",
    "estimators.append(('Logistic Regression',clf3))\n",
    "ensemble = VotingClassifier(estimators, voting='soft', weights=[2,1,1])\n",
    "ensemble=ensemble.fit(text_train_en,targets_train)\n",
    "prediction_ensemble=ensemble.predict(text_test_en)\n",
    "ensemble_score = f1_score(targets_test, prediction_ensemble, average='macro')\n",
    "print 'ensemble results:', ensemble_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of results using various models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEZCAYAAACervI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niBoNM1I0avcAAAAASUVORK5CYII=\n"
     },
     "output_type": "display_data",
     "text": [
      "<matplotlib.figure.Figure at 0xcb63940>"
     ],
     "metadata": {}
    }
   ],
   "source": [
    "#Plot showing results of models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "scores = [baseline,score_logit,random_score,svc_score,ensemble_score]\n",
    "models = ['Baseline','Logit','RandomForest','SVM','Ensemble']\n",
    "y_pos=np.arange(len(scores))\n",
    "my_colors = 'rgbkymc'\n",
    "plt.bar(y_pos,scores, alpha=0.5,align='center',color=my_colors)\n",
    "plt.xticks(y_pos,models)\n",
    "plt.ylabel('Score');\n",
    "plt.xlabel('Classifiers')\n",
    "plt.title('F1 macro score for different models')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}